{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on our project\n",
    "\n",
    "* **Motivation**\n",
    "\n",
    "    * What is your dataset?\n",
    "    * Why did you choose this/these particular dataset(s)? \n",
    "    * What was your goal for the end user's experience?\n",
    "    \n",
    "    All the data is downloaded from [Game of Thrones Wiki](http://gameofthrones.wikia.com/wiki/Game_of_Thrones_Wiki). This is a Wiki page that contains a lot of explanations about characters, episodes etc. We used the API system of the page to get acess to the different pages. Our dataset consists of 1152 files of the GTO characters' wiki pages, 89 files of the episodes' content, and a txt file of all the characters' ids, names and episodes' names.\n",
    "    \n",
    "    We choose this dataset because we both like the Game of Thrones series. It is known that there are many complicate social relationships in the series, and it will be cool if we play round with data from the wiki website! Besides, it will be rather cool to see how the sentiment changes over time and probably predict the result of an episode. And we can get what we want using the API, which is really handy to use.\n",
    "    \n",
    "    As for our goal, \n",
    "        * firstly, we aim to find the relationships of the GOT characters, assign some attributes such as dead/alive and houses to them, and analyze their graph characteristics and communities. \n",
    "        * Then, we plan to do some basic NLTK analysis like word cloud on the episodes. We will also find how the sentiment changes over the episodes and inside each episodes. We give each episode negative, positive and neutral scores.\n",
    "        * We also looked at how often and where a character appears in an episode, and see if it has some relations with the episodes' sentiment.\n",
    "        * Using the sentiment score of each character, we define a way to give each character a *sadness score*, and we use this score to predict an episode's sentiment outcome (score).\n",
    "    \n",
    "    \n",
    "* **Basic stats. Let's understand the dataset better**\n",
    "\n",
    "    * Write about your choices in data cleaning and preprocessing\n",
    "    * Write a short section that discusses the dataset stats\n",
    "    \n",
    "    We used the character id to access the different pages. It's like something we've done in the philosopher task. For the characters, when we download their pages, we replace \"/\" with \"\\_\" (since / can't be used in a file name), and get the lower characters. We get the json files, but it still have some html languages. To get a person's sttributes like status, we use natural language like r'(?<=Status:\\_)(.\\*?)(?=\\\")' to match. Besides, when working on episodes, we need clean texts. We use BeautifulSoup library to get all the raw text for later use.\n",
    "    \n",
    "    The total size of our dataset is about 27.9 MB. \n",
    "        * The nodes of the network is simply all the characters. There are 1057 different characters and thus 1057 nodes. The network has been trimmed by removing all the nodes without any edges. This leaves 1013 nodes. \n",
    "        * We created both the directed graph and the undirected graph. Directed graph has 9159 edges. Undirected graph has 6756 edges\n",
    "        * We give each character the 2 attributes. Status: Attribute that tells if the character is dead or alive in the series at the time of writing. House/Allegiance: Attribute that tells what house, person or other group that the character has sworn allegiance to.\n",
    "        * We have 65 episodes' contents.\n",
    "    \n",
    "    \n",
    "* **Tools, theory and analysis. Describe the process of theory to insight**\n",
    "\n",
    "    * Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "    * Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "    * How did you use the tools to understand your dataset?\n",
    "    \n",
    "    We stored each character's id in unicode form, and encoded each file in 'utf-8' style. To get a person's sttributes like status, we use natural language like r'(?<=Status:\\_)(.\\*?)(?=\\\")' to match. Besides, when working on episodes, we need clean texts. We use BeautifulSoup library to get all the raw text for later use. In the episodes' names file, there are some names we are not interested in, such as some recaps and some file named like \"Season 1\". We use a condition function to see if the name's last letters are \"recap\" or the first letters are \"Season\". To get the sentiment score, we use the LabMT.txt file from this [paper](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752), and defined a function to calculate.\n",
    "    \n",
    "    Network science tools: \n",
    "        * Basic insight on nodes, edges and node attributes.\n",
    "        * Community analysis, both using package and manually. And used a confusion matrix.\n",
    "        * In-degree and out-degree distribution, distribution plot(regular and loglog style), and compared that with power law distribution.\n",
    "        * Betweeness centrality and eigenvector centrality.\n",
    "    \n",
    "    We just used these analysis method as we did in the course, and luckily this network is perfect for implementing. We can really find people's relationships inside and between houses, and we can see that the characters who have the highest degree are indeed the most important ones.\n",
    "    \n",
    "    \n",
    "* **Discussion. Think critically about your creation**\n",
    "    * What went well?\n",
    "    * What is still missing? What could be improved?, Why?\n",
    "    \n",
    "    Things went well is that we managed to follow all our plans. We did all the analysis on the network, see something from the network about the most important characters, their alliances. And got some very useful and neat plots, also some good analysis. The part I like best is that we made many assumptions, and we found our own ways to test our assumptions. For example, we successfully plotted the characters' dispersion plot together with the sentiment profile to see if some characters' appearance affects the episode's sentiment.\n",
    "    \n",
    "    The problem is that we sometimes don't get a very well-performed result. For example, we assume that characters who have dead may also have a lower sentiment score, but things turned out that this is not correlated. Besides, we assume if we consider which characters appear in an episode and get a weighted sum of their sentiment score, then we can predict the sentiment score of an episode. But things turn out that this prediction is not so successful. If we use more machine learning tools, we may get a better prediction result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
